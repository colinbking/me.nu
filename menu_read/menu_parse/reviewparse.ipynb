{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import time as t\n",
    "from lxml import html  \n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from itertools import cycle\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/michaelsprintson/Documents/GitHub/me.nu/menu_read\n"
     ]
    }
   ],
   "source": [
    "cd ~/Documents/GitHub/me.nu/menu_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "food = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ocr\\\\words_alpha.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8c882625f2c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mrespage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://www.yelp.com/biz/mala-sichuan-bistro-houston-3'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mswitch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'food'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mmenu_read\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mocr_food\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mocr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mmenu_read\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjust_read_food\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ChIJNc4K5cTCQIYRe9OyIN7DcGE'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/me.nu/menu_read/menu_read.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;31m# create dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0mpref_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"menu_read\\\\pref_sample.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/me.nu/menu_read/menu_read.py\u001b[0m in \u001b[0;36mload_words\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"ocr\\words_alpha.txt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mword_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mvalid_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalid_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ocr\\\\words_alpha.txt'"
     ]
    }
   ],
   "source": [
    "if food:\n",
    "    respage = 'https://www.yelp.com/biz/mala-sichuan-bistro-houston-3'\n",
    "    switch = 'food'\n",
    "    import menu_read.ocr_food as ocr\n",
    "    import menu_read.just_read_food as jr\n",
    "    pid = 'ChIJNc4K5cTCQIYRe9OyIN7DcGE'\n",
    "else:\n",
    "    respage = 'https://www.yelp.com/biz/sharetea-houston-2'\n",
    "    switch = 'tea'\n",
    "    import menu_read.ocr_tea as ocr\n",
    "    import menu_read.just_read_tea as jr\n",
    "    pid = 'ChIJjfzjCM_CQIYRPA546CYaE4A'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR FUNCTIONALITY TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade google-cloud-vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary\n",
    "pic_loc = 'ocr/menupictures/pic7.jpg'\n",
    "test_file_name = 'mala7'\n",
    "ocr.detect_text(pic_loc, test_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jr.final_dump(\"ocr/menu_tests/mala7.txt\", \"pref_sample.txt\", True, \"mala7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT RESTERAUNT REVIEWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT YELP REVIEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reviews_df=pd.DataFrame()\n",
    "  \n",
    "user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0 Safari/605.1.15'\n",
    "webpage = ''\n",
    "nextpage = 'something'\n",
    "xpath_reviews = '//script[@type=\"application/ld+json\"]/text()'\n",
    "xpath_nextpage = '//link[@rel=\"next\"]'\n",
    "headers = {'User-Agent': user_agent}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if food:\n",
    "    reviewdf = pd.read_json('menu_parse/bigmala')\n",
    "elif not food:\n",
    "    reviewdf = pd.read_json('menu_parse/sharetea')\n",
    "else:\n",
    "    reviewdf = pd.DataFrame()\n",
    "\n",
    "    while len(nextpage) > 0:\n",
    "        #print('1')\n",
    "        page = requests.get(webpage, headers = headers)#,proxies={\"http\": proxy, \"https\": proxy})\n",
    "        print('pagegot','\\r')\n",
    "        parser = html.fromstring(page.content)\n",
    "        reviews = parser.xpath(xpath_reviews)\n",
    "        nextpage = parser.xpath(xpath_nextpage)\n",
    "\n",
    "        if len(nextpage) > 0:\n",
    "            nextlink = nextpage[0].get('href')\n",
    "            print(nextlink, end = '\\r')\n",
    "            webpage = nextlink\n",
    "            y = json.loads(list(reviews)[0])['review']\n",
    "\n",
    "            reviewdict = {y.index(i):i for i in y}\n",
    "\n",
    "            reviewdf = pd.concat([reviewdf,pd.DataFrame(reviewdict).T])\n",
    "            \n",
    "    s = lambda x: list(x.values())[0]\n",
    "    reviewdf['reviewRating'] = reviewdf['reviewRating'].apply(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT GOOGLE REVIEWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### todo: make place id google maps api modular through api call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import googlemaps\n",
    "from datetime import datetime\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmapsapikey = json.load(open('menu_parse/gmapsapikey.json', 'r'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f'https://maps.googleapis.com/maps/api/place/details/json?key={gmapsapikey}&place_id={pid}&fields=name,review'\n",
    "response = requests.get(url, headers=headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googlereviewdict = json.loads(response.content.decode('utf-8'))['result']['reviews']\n",
    "googlereviewdict = {googlereviewdict.index(i):i for i in googlereviewdict}\n",
    "\n",
    "googlereviewdf = pd.DataFrame(googlereviewdict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googlereviewdf = googlereviewdf[['author_name','rating','text']]\n",
    "googlereviewdf.columns = ['author','reviewRating','description']\n",
    "googlereviewdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewdf = pd.concat([reviewdf,googlereviewdf],sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewdf = reviewdf.reset_index().drop(['index'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviewdf['reviewRating'] = reviewdf['reviewRating'].apply(lambda x:x['ratingValue'])\n",
    "reviewdf['description'] = reviewdf['description'].apply(lambda x:x.replace('\\n','').replace('-',' ').lower())\n",
    "reviewdf['description'] = reviewdf['description'].apply(lambda x:' '.join([i for i in x.split(\" \") if len(i)>2]))\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "reviewdf['description'] = reviewdf['description'].apply(lambda x:x.translate(translator)).apply(lambda x: ' '.join(x.split()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRAB MENU ITEMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "menuitems = json.load(open('menuJSON/mala7.json'))\n",
    "if food:\n",
    "    menuitems = [x[3:].strip() for x in list(menuitems.keys())]\n",
    "menitems = [x.lower() for x in menuitems]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANKING MENU ITEMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sanity check of menu items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "menitems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = lambda x: [menitem for menitem in menitems if menitem[:math.floor(0.8*len(menitem))] in x]\n",
    "reviewdf['containedMenuItems'] = reviewdf['description'].apply(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "menitemstoreviews = {}\n",
    "for menitem in menitems:\n",
    "    menitemstoreviews[menitem] = [{},list(reviewdf[reviewdf['containedMenuItems'].map(lambda d: menitem in d)].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot4plusreviews = len(reviewdf[reviewdf['reviewRating'] >= 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total number of 4 plus reviews\n",
    "for item in menitemstoreviews:\n",
    "    #print(item,'------',end = '')\n",
    "    allratings = []\n",
    "    fourpluscounter = 0\n",
    "    twominuscounter = 0\n",
    "    totalrev = len(menitemstoreviews[item][1])\n",
    "    \n",
    "    for rev in menitemstoreviews[item][1]:\n",
    "        curstar = reviewdf.iloc[rev]['reviewRating']\n",
    "        allratings.append(curstar)\n",
    "        if curstar >= 4:\n",
    "            fourpluscounter += 1\n",
    "        elif curstar <= 2:\n",
    "            twominuscounter += 1\n",
    "    if not totalrev == 0:\n",
    "        menitemstoreviews[item][0]['extremerev'] = (fourpluscounter - twominuscounter) / (2 * totalrev)\n",
    "    else:\n",
    "        menitemstoreviews[item][0]['extremerev'] = 0\n",
    "    menitemstoreviews[item][0]['percentageoftotalreviews'] = 8.5 * (fourpluscounter) / tot4plusreviews\n",
    "        \n",
    "    if not len(allratings) == 0:\n",
    "        menitemstoreviews[item][0]['avgrev'] = sum(allratings)/(5 * len(allratings))\n",
    "        #print('av rating',sum(allratings)/len(allratings))\n",
    "    else:\n",
    "        menitemstoreviews[item][0]['avgrev'] = 0\n",
    "        #print('no reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemratings = pd.DataFrame(menitemstoreviews).T\n",
    "itemratings.columns = ['ratings','indexes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemratings['totalscore'] = itemratings['ratings'].apply(lambda x: sum(x.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "itemratings['totalscore'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIP / TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itemratings.loc['water boiled beef']['ratings']\n",
    "\n",
    "# reviewdf[reviewdf['description'].map(lambda d: 'okinawa pearl milk tea' in d)]\n",
    "\n",
    "\n",
    "\n",
    "# reviewdf.iloc[548]['description']\n",
    "\n",
    "\n",
    "\n",
    "# pd.DataFrame(menitemstoreviews).T\n",
    "\n",
    "# reviewdf[reviewdf['author']=='Tristan & Ashley Boyd']\n",
    "\n",
    "# reviewdf.iloc[473]['description']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #reviewdf.to_json('bigmala')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USER PREFERENCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scrape wikipedia for chinese foods (only run once, save results as JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wikipage = 'https://en.wikipedia.org/wiki/List_of_Chinese_dishes'\n",
    "wikipage = requests.get(wikipage).text\n",
    "wikidf = pd.DataFrame()\n",
    "\n",
    "soup = BeautifulSoup(wikipage)\n",
    "\n",
    "my_tables = soup.findAll('table',{'class':'wikitable'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinesefoods = []\n",
    "for table in my_tables:\n",
    "    alla = list(table.findAll('a'))\n",
    "    chinesefoods = chinesefoods +[x for x in[alla[i].get('title') for i in range(len(alla))] if type(x) == str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "allfoods = list(set([w for s in wn.synset('food.n.02').closure(lambda s:s.hyponyms()) for w in s.lemma_names()])) + chinesefoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(allfoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## food preferences by users reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = lambda x: [food for food in allfoods if food in x]\n",
    "\n",
    "userreviewdf['containedFood='] = userreviewdf['description'].apply(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food = [x.lower() for x in food]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in menuitems:\n",
    "    test = i.split(\" \")\n",
    "    test = [x.lower() for x in test]\n",
    "    print(i)\n",
    "    for j in test:\n",
    "        if j in food:\n",
    "            print ('y',j)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT USER REVIEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grabfullreviws(ureviews):\n",
    "    fullreviews = []\n",
    "    curreview = ''\n",
    "    for i in range(len(ureviews)):\n",
    "        #print('\\n')\n",
    "        if not (i == len(ureviews)-1):\n",
    "            if not type(ureviews[i]) == str:\n",
    "                #print('not a review',ureviews[i],type(ureviews[i]))\n",
    "                pass\n",
    "            elif not type(ureviews[i+1]) == str:\n",
    "                #print('middle of a review',ureviews[i],type(ureviews[i]))\n",
    "                curreview = curreview + ureviews[i]\n",
    "            elif type(ureviews[i+1]) == str:\n",
    "                #print('end of a review',ureviews[i],type(ureviews[i]))\n",
    "                curreview = curreview + ureviews[i]\n",
    "                fullreviews.append(curreview)\n",
    "                curreview = ''\n",
    "        else:\n",
    "            curreview = curreview + ureviews[i]\n",
    "            fullreviews.append(curreview)\n",
    "    return fullreviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unextpage = 'something'\n",
    "userpage = 'https://www.yelp.com/user_details_reviews_self?rec_pagestart=0&userid=2fKJeKlPi9le_ta7DPVW_A'\n",
    "userreviewdf = pd.DataFrame()\n",
    "\n",
    "upath_reviews = '//p[@lang=\"en\"]//node()'\n",
    "upath_nextpage = '//a[@class=\"u-decoration-none next pagination-links_anchor\"]'\n",
    "upath_starrating = '//div[@class=\"biz-rating__stars\"]//div'\n",
    "\n",
    "while len(unextpage) > 0:\n",
    "\n",
    "    userpage = requests.get(userpage, headers = headers)#,proxies={\"http\": proxy, \"https\": proxy})\n",
    "    \n",
    "    uparser = html.fromstring(userpage.content)\n",
    "\n",
    "    ureviews = uparser.xpath(upath_reviews)\n",
    "\n",
    "    unextpage = uparser.xpath(upath_nextpage)\n",
    "    \n",
    "    if len(unextpage) > 0:\n",
    "        print(unextpage)\n",
    "        userpage = unextpage[0].get('href')\n",
    "    ustarratingpath = uparser.xpath(upath_starrating)\n",
    "\n",
    "    ureviews = [str(i) if not \"Element br\" in str(i) else 0 for i in ureviews ]\n",
    "\n",
    "    ufullreviews = grabfullreviws(ureviews)\n",
    "\n",
    "    fullstars = [i.get('title') for i in ustarratingpath]\n",
    "\n",
    "    userreviewdf = pd.concat([userreviewdf,pd.DataFrame(list(zip(fullstars,ufullreviews)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userreviewdf.columns = ['rating','description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userreviewdf.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
